{"cells":[{"cell_type":"markdown","metadata":{"id":"Vkkr1Sq6t2lM"},"source":["#◢ Bringing Old Photos Back to Life"]},{"cell_type":"markdown","metadata":{"id":"ypb6kal06Tb1"},"source":["This is a reference implementation of our CVPR 2020 paper [1], which  revives an old photo to modern style. Should you be making use of our work, please cite our paper [1]."]},{"cell_type":"markdown","metadata":{"id":"IwXBx7z6rfXK"},"source":["\n","\n","---\n","\n","\n","#◢ Verify Runtime Settings\n","\n","**\u003cfont color='#FF000'\u003e IMPORTANT \u003c/font\u003e**\n","\n","In the \"Runtime\" menu for the notebook window, select \"Change runtime type.\" Ensure that the following are selected:\n","* Runtime Type = Python 3\n","* Hardware Accelerator = GPU\n"]},{"cell_type":"markdown","metadata":{"id":"ZMZ2EAlBrvkq"},"source":["#◢ Git clone\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5311,"status":"ok","timestamp":1735624894382,"user":{"displayName":"sunny anand","userId":"02120450761526046192"},"user_tz":-330},"id":"69H2guBfrzqu","outputId":"bf50c260-dccc-490f-eb69-d74aaebe938b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'photo_restoration'...\n","remote: Enumerating objects: 509, done.\u001b[K\n","remote: Counting objects: 100% (242/242), done.\u001b[K\n","remote: Compressing objects: 100% (68/68), done.\u001b[K\n","remote: Total 509 (delta 187), reused 174 (delta 174), pack-reused 267 (from 1)\u001b[K\n","Receiving objects: 100% (509/509), 40.88 MiB | 15.18 MiB/s, done.\n","Resolving deltas: 100% (240/240), done.\n"]}],"source":["!git clone https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life.git photo_restoration"]},{"cell_type":"markdown","metadata":{"id":"Ubc05fcKzk90"},"source":["#◢ Set up the environment\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143816,"status":"ok","timestamp":1735625038193,"user":{"displayName":"sunny anand","userId":"02120450761526046192"},"user_tz":-330},"id":"32jCofdSr8AW","outputId":"ecf71b01-5593-4ea2-9982-6e56709c5a15"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/photo_restoration/Face_Enhancement/models/networks\n","Cloning into 'Synchronized-BatchNorm-PyTorch'...\n","remote: Enumerating objects: 188, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 188 (delta 10), reused 27 (delta 10), pack-reused 161 (from 1)\u001b[K\n","Receiving objects: 100% (188/188), 47.20 KiB | 2.36 MiB/s, done.\n","Resolving deltas: 100% (106/106), done.\n","/content/photo_restoration\n","/content/photo_restoration/Global/detection_models\n","Cloning into 'Synchronized-BatchNorm-PyTorch'...\n","remote: Enumerating objects: 188, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 188 (delta 10), reused 27 (delta 10), pack-reused 161 (from 1)\u001b[K\n","Receiving objects: 100% (188/188), 47.20 KiB | 3.15 MiB/s, done.\n","Resolving deltas: 100% (106/106), done.\n","/content/photo_restoration\n","/content/photo_restoration/Face_Detection\n","--2024-12-31 06:01:38--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Resolving dlib.net (dlib.net)... 107.180.26.78\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 64040097 (61M)\n","Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n","\n","shape_predictor_68_ 100%[===================\u003e]  61.07M  18.1MB/s    in 4.2s    \n","\n","2024-12-31 06:01:42 (14.6 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n","\n","/content/photo_restoration\n","/content/photo_restoration/Face_Enhancement\n","--2024-12-31 06:01:48--  https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/face_checkpoints.zip\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/49cb1e00-e34c-11eb-82bf-3c592a7d16da?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20241231T060149Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=8ccdae4ddee479cab9d227a9e9f81054b963c322cdc6474897c83543c5b35ff8\u0026X-Amz-SignedHeaders=host\u0026response-content-disposition=attachment%3B%20filename%3Dface_checkpoints.zip\u0026response-content-type=application%2Foctet-stream [following]\n","--2024-12-31 06:01:49--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/49cb1e00-e34c-11eb-82bf-3c592a7d16da?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20241231T060149Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=8ccdae4ddee479cab9d227a9e9f81054b963c322cdc6474897c83543c5b35ff8\u0026X-Amz-SignedHeaders=host\u0026response-content-disposition=attachment%3B%20filename%3Dface_checkpoints.zip\u0026response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 684354563 (653M) [application/octet-stream]\n","Saving to: ‘face_checkpoints.zip’\n","\n","face_checkpoints.zi 100%[===================\u003e] 652.65M  29.2MB/s    in 23s     \n","\n","2024-12-31 06:02:12 (28.5 MB/s) - ‘face_checkpoints.zip’ saved [684354563/684354563]\n","\n","Archive:  face_checkpoints.zip\n","   creating: checkpoints/\n","   creating: checkpoints/Setting_9_epoch_100/\n","  inflating: checkpoints/Setting_9_epoch_100/latest_net_G.pth  \n","   creating: checkpoints/FaceSR_512/\n","  inflating: checkpoints/FaceSR_512/latest_net_G.pth  \n","/content/photo_restoration\n","/content/photo_restoration/Global\n","--2024-12-31 06:02:22--  https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/global_checkpoints.zip\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/75e69f00-e34c-11eb-9435-335b7429f0a1?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20241231T060223Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=1c86e7b0855ac739a38bb6af93e08f4698d75775cd3455c850ec13f866ad9cd9\u0026X-Amz-SignedHeaders=host\u0026response-content-disposition=attachment%3B%20filename%3Dglobal_checkpoints.zip\u0026response-content-type=application%2Foctet-stream [following]\n","--2024-12-31 06:02:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/75e69f00-e34c-11eb-9435-335b7429f0a1?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20241231T060223Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=1c86e7b0855ac739a38bb6af93e08f4698d75775cd3455c850ec13f866ad9cd9\u0026X-Amz-SignedHeaders=host\u0026response-content-disposition=attachment%3B%20filename%3Dglobal_checkpoints.zip\u0026response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2036400762 (1.9G) [application/octet-stream]\n","Saving to: ‘global_checkpoints.zip’\n","\n","global_checkpoints.  27%[====\u003e               ] 531.25M  25.8MB/s    in 20s     \n","\n","2024-12-31 06:02:44 (26.2 MB/s) - Read error at byte 557052302/2036400762 (Connection reset by peer). Retrying.\n","\n","--2024-12-31 06:02:45--  (try: 2)  https://objects.githubusercontent.com/github-production-release-asset-2e65be/274594200/75e69f00-e34c-11eb-9435-335b7429f0a1?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=releaseassetproduction%2F20241231%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20241231T060223Z\u0026X-Amz-Expires=300\u0026X-Amz-Signature=1c86e7b0855ac739a38bb6af93e08f4698d75775cd3455c850ec13f866ad9cd9\u0026X-Amz-SignedHeaders=host\u0026response-content-disposition=attachment%3B%20filename%3Dglobal_checkpoints.zip\u0026response-content-type=application%2Foctet-stream\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 206 Partial Content\n","Length: 2036400762 (1.9G), 1479348460 (1.4G) remaining [application/octet-stream]\n","Saving to: ‘global_checkpoints.zip’\n","\n","global_checkpoints. 100%[+++++==============\u003e]   1.90G  29.9MB/s    in 49s     \n","\n","2024-12-31 06:03:34 (28.9 MB/s) - ‘global_checkpoints.zip’ saved [2036400762/2036400762]\n","\n","Archive:  global_checkpoints.zip\n","   creating: checkpoints/\n","   creating: checkpoints/restoration/\n","   creating: checkpoints/restoration/VAE_B_scratch/\n","  inflating: checkpoints/restoration/VAE_B_scratch/latest_net_G.pth  \n","  inflating: checkpoints/restoration/VAE_B_scratch/latest_optimizer_G.pth  \n","  inflating: checkpoints/restoration/VAE_B_scratch/latest_optimizer_D.pth  \n","  inflating: checkpoints/restoration/VAE_B_scratch/latest_net_D.pth  \n","   creating: checkpoints/restoration/VAE_A_quality/\n","  inflating: checkpoints/restoration/VAE_A_quality/latest_net_G.pth  \n","  inflating: checkpoints/restoration/VAE_A_quality/latest_net_featD.pth  \n","  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_G.pth  \n","  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_D.pth  \n","  inflating: checkpoints/restoration/VAE_A_quality/latest_optimizer_featD.pth  \n","  inflating: checkpoints/restoration/VAE_A_quality/latest_net_D.pth  \n","   creating: checkpoints/restoration/mapping_Patch_Attention/\n","  inflating: checkpoints/restoration/mapping_Patch_Attention/latest_net_mapping_net.pth  \n","  inflating: checkpoints/restoration/mapping_Patch_Attention/latest_net_D.pth  \n","   creating: checkpoints/restoration/mapping_quality/\n","  inflating: checkpoints/restoration/mapping_quality/latest_net_mapping_net.pth  \n","  inflating: checkpoints/restoration/mapping_quality/latest_optimizer_mapping_net.pth  \n","  inflating: checkpoints/restoration/mapping_quality/latest_optimizer_D.pth  \n","  inflating: checkpoints/restoration/mapping_quality/latest_net_D.pth  \n","   creating: checkpoints/restoration/mapping_scratch/\n","  inflating: checkpoints/restoration/mapping_scratch/latest_net_mapping_net.pth  \n","  inflating: checkpoints/restoration/mapping_scratch/latest_optimizer_mapping_net.pth  \n","  inflating: checkpoints/restoration/mapping_scratch/latest_optimizer_D.pth  \n","  inflating: checkpoints/restoration/mapping_scratch/latest_net_D.pth  \n","   creating: checkpoints/restoration/VAE_B_quality/\n","  inflating: checkpoints/restoration/VAE_B_quality/latest_net_G.pth  \n","  inflating: checkpoints/restoration/VAE_B_quality/latest_optimizer_G.pth  \n","  inflating: checkpoints/restoration/VAE_B_quality/latest_optimizer_D.pth  \n","  inflating: checkpoints/restoration/VAE_B_quality/latest_net_D.pth  \n","   creating: checkpoints/detection/\n","  inflating: checkpoints/detection/FT_Epoch_latest.pt  \n","/content/photo_restoration\n"]}],"source":["# pull the syncBN repo\n","%cd photo_restoration/Face_Enhancement/models/networks\n","!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n","!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n","%cd ../../../\n","\n","%cd Global/detection_models\n","!git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n","!cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n","%cd ../../\n","\n","# download the landmark detection model\n","%cd Face_Detection/\n","!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n","%cd ../\n","\n","# download the pretrained model\n","%cd Face_Enhancement/\n","#!wget https://facevc.blob.core.windows.net/zhanbo/old_photo/pretrain/Face_Enhancement/checkpoints.zip\n","!wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/face_checkpoints.zip\n","!unzip face_checkpoints.zip\n","#!unzip checkpoints.zip\n","%cd ../\n","\n","%cd Global/\n","# !wget https://facevc.blob.core.windows.net/zhanbo/old_photo/pretrain/Global/checkpoints.zip\n","# !unzip checkpoints.zip\n","!wget https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life/releases/download/v1.0/global_checkpoints.zip\n","!unzip global_checkpoints.zip\n","%cd ../"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3511,"status":"ok","timestamp":1735625041699,"user":{"displayName":"sunny anand","userId":"02120450761526046192"},"user_tz":-330},"id":"B3v8tvmtw85c","outputId":"10de4a35-4580-48d7-fcbc-8562f04d6ecc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n","Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (19.24.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.25.0)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.13)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (6.0.2)\n","Collecting dominate\u003e=2.3.1 (from -r requirements.txt (line 7))\n","  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting dill (from -r requirements.txt (line 8))\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Collecting tensorboardX (from -r requirements.txt (line 9))\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.13.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.10.0.84)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.8.0)\n","Collecting PySimpleGUI (from -r requirements.txt (line 13))\n","  Downloading PySimpleGUI-5.0.7-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch-\u003e-r requirements.txt (line 1)) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch-\u003e-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision-\u003e-r requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision-\u003e-r requirements.txt (line 2)) (11.0.0)\n","Requirement already satisfied: imageio!=2.35.0,\u003e=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image-\u003e-r requirements.txt (line 4)) (2.36.1)\n","Requirement already satisfied: tifffile\u003e=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image-\u003e-r requirements.txt (line 4)) (2024.12.12)\n","Requirement already satisfied: packaging\u003e=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image-\u003e-r requirements.txt (line 4)) (24.2)\n","Requirement already satisfied: lazy-loader\u003e=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image-\u003e-r requirements.txt (line 4)) (0.4)\n","Requirement already satisfied: protobuf\u003e=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX-\u003e-r requirements.txt (line 9)) (4.25.5)\n","Requirement already satisfied: rsa in /usr/local/lib/python3.10/dist-packages (from PySimpleGUI-\u003e-r requirements.txt (line 13)) (4.9)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (1.3.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (4.55.3)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (1.4.7)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (3.2.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 14)) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003e-r requirements.txt (line 14)) (1.17.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch-\u003e-r requirements.txt (line 1)) (3.0.2)\n","Requirement already satisfied: pyasn1\u003e=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa-\u003ePySimpleGUI-\u003e-r requirements.txt (line 13)) (0.6.1)\n","Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n","Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PySimpleGUI-5.0.7-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX, dominate, dill, PySimpleGUI\n","Successfully installed PySimpleGUI-5.0.7 dill-0.3.9 dominate-2.9.1 tensorboardX-2.6.2.2\n"]}],"source":["! pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"soHBzgRU8rPY"},"source":["#◢ Run the code"]},{"cell_type":"markdown","metadata":{"id":"EVpoONmCcJDt"},"source":["### Restore photos (normal mode)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94063,"status":"ok","timestamp":1735625135759,"user":{"displayName":"sunny anand","userId":"02120450761526046192"},"user_tz":-330},"id":"q6lNy6jw5rjd","outputId":"2931d4ab-1aca-416e-c65f-d9f16ddeff0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/photo_restoration\n","Running Stage 1: Overall restoration\n","Mapping: You are using the mapping model without global restoration.\n","/content/photo_restoration/Global/models/base_model.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  network.load_state_dict(torch.load(save_path))\n","Now you are processing a.png\n","Now you are processing b.png\n","Now you are processing c.png\n","Now you are processing d.png\n","Now you are processing e.png\n","Now you are processing f.png\n","Now you are processing g.png\n","Now you are processing h.png\n","Finish Stage 1 ...\n","\n","\n","Running Stage 2: Face Detection\n","Warning: There is no face in d.png\n","1\n","1\n","1\n","Warning: There is no face in f.png\n","Warning: There is no face in e.png\n","Warning: There is no face in b.png\n","1\n","Finish Stage 2 ...\n","\n","\n","Running Stage 3: Face Enhancement\n","The main GPU is \n","0\n","dataset [FaceTestDataset] of size 4 was created\n","The size of the latent vector size is [8,8]\n","Network [SPADEGenerator] was created. Total number of parameters: 92.1 million. To see the architecture, do print(network).\n","/content/photo_restoration/Face_Enhancement/util/util.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  weights = torch.load(save_path)\n","hi :)\n","Finish Stage 3 ...\n","\n","\n","Running Stage 4: Blending\n","Warning: There is no face in d.png\n","Traceback (most recent call last):\n","  File \"/content/photo_restoration/Face_Detection/align_warp_back_multiple_dlib.py\", line 428, in \u003cmodule\u003e\n","    blended = blur_blending_cv2(warped_back, blended, backward_mask)\n","  File \"/content/photo_restoration/Face_Detection/align_warp_back_multiple_dlib.py\", line 219, in blur_blending_cv2\n","    mask *= 255.0\n","numpy.core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'\n","Finish Stage 4 ...\n","\n","\n","All the processing is done. Please check the results.\n"]}],"source":["%cd /content/photo_restoration/\n","input_folder = \"test_images/old\"\n","output_folder = \"output\"\n","\n","import os\n","basepath = os.getcwd()\n","input_path = os.path.join(basepath, input_folder)\n","output_path = os.path.join(basepath, output_folder)\n","os.mkdir(output_path)\n","\n","!python run.py --input_folder /content/photo_restoration/test_images/old --output_folder /content/photo_restoration/output/ --GPU 0"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1735625135760,"user":{"displayName":"sunny anand","userId":"02120450761526046192"},"user_tz":-330},"id":"6yeeDiM4exHz"},"outputs":[],"source":["import io\n","import IPython.display\n","import numpy as np\n","import PIL.Image\n","\n","def imshow(a, format='png', jpeg_fallback=True):\n","    a = np.asarray(a, dtype=np.uint8)\n","    data = io.BytesIO()\n","    PIL.Image.fromarray(a).save(data, format)\n","    im_data = data.getvalue()\n","    try:\n","      disp = IPython.display.display(IPython.display.Image(im_data))\n","    except IOError:\n","      if jpeg_fallback and format != 'jpeg':\n","        print(('Warning: image was too large to display in format \"{}\"; '\n","              'trying jpeg instead.').format(format))\n","        return imshow(a, format='jpeg')\n","      else:\n","        raise\n","    return disp\n","\n","def make_grid(I1, I2, resize=True):\n","    I1 = np.asarray(I1)\n","    H, W = I1.shape[0], I1.shape[1]\n","\n","    if I1.ndim \u003e= 3:\n","        I2 = np.asarray(I2.resize((W,H)))\n","        I_combine = np.zeros((H,W*2,3))\n","        I_combine[:,:W,:] = I1[:,:,:3]\n","        I_combine[:,W:,:] = I2[:,:,:3]\n","    else:\n","        I2 = np.asarray(I2.resize((W,H)).convert('L'))\n","        I_combine = np.zeros((H,W*2))\n","        I_combine[:,:W] = I1[:,:]\n","        I_combine[:,W:] = I2[:,:]\n","    I_combine = PIL.Image.fromarray(np.uint8(I_combine))\n","\n","    W_base = 600\n","    if resize:\n","      ratio = W_base / (W*2)\n","      H_new = int(H * ratio)\n","      I_combine = I_combine.resize((W_base, H_new), PIL.Image.LANCZOS)\n","\n","    return I_combine"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":505,"output_embedded_package_id":"1i4M8nJVJUN7cD7W-lQJjWY19gWiaiJ4l"},"id":"u_Eo4Hjti7Nh","outputId":"47946a0e-b355-42d7-a088-0087296ecab2"},"outputs":[],"source":["filenames = os.listdir(os.path.join(input_path))\n","filenames.sort()\n","\n","for filename in filenames:\n","    print(filename)\n","    image_original = PIL.Image.open(os.path.join(input_path, filename))\n","    image_restore = PIL.Image.open(os.path.join(output_path, 'final_output', filename))\n","\n","    display(make_grid(image_original, image_restore))"]},{"cell_type":"markdown","metadata":{"id":"tSUF96UgTuwd"},"source":["### Restore the photos with scratches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-yb3lO5T8aM"},"outputs":[],"source":["!rm -rf /content/photo_restoration/output/*\n","!python run.py --input_folder /content/photo_restoration/test_images/old_w_scratch/ --output_folder /content/photo_restoration/output/ --GPU 0 --with_scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSSORPEMUSH0"},"outputs":[],"source":["input_folder = \"test_images/old_w_scratch\"\n","output_folder = \"output\"\n","input_path = os.path.join(basepath, input_folder)\n","output_path = os.path.join(basepath, output_folder)\n","\n","filenames = os.listdir(os.path.join(input_path))\n","filenames.sort()\n","\n","for filename in filenames:\n","    print(filename)\n","    image_original = PIL.Image.open(os.path.join(input_path, filename))\n","    image_restore = PIL.Image.open(os.path.join(output_path, 'final_output', filename))\n","\n","    display(make_grid(image_original, image_restore))"]},{"cell_type":"markdown","metadata":{"id":"LMnje_NWj24x"},"source":["#◢ Try it on your own photos!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vov9hg957-D"},"outputs":[],"source":["from google.colab import files\n","import shutil\n","\n","upload_path = os.path.join(basepath, \"test_images\", \"upload\")\n","upload_output_path = os.path.join(basepath, \"upload_output\")\n","\n","if os.path.isdir(upload_output_path):\n","    shutil.rmtree(upload_output_path)\n","\n","if os.path.isdir(upload_path):\n","    shutil.rmtree(upload_path)\n","\n","os.mkdir(upload_output_path)\n","os.mkdir(upload_path)\n","\n","uploaded = files.upload()\n","for filename in uploaded.keys():\n","    shutil.move(os.path.join(basepath, filename), os.path.join(upload_path, filename))"]},{"cell_type":"markdown","metadata":{"id":"cy9vSWTHMH5U"},"source":["Run the processing with:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgRUwTqsjr7m"},"outputs":[],"source":["!python run.py --input_folder /content/photo_restoration/test_images/upload --output_folder /content/photo_restoration/upload_output --GPU 0 --with_scratch --HR"]},{"cell_type":"markdown","metadata":{"id":"_lEXtwXpLl1L"},"source":["### Visualize\n","\n","Now you have all your results under the folder `upload_output` and you can *manually* right click and download them.\n","\n","Here we use the child photos of celebrities from https://www.boredpanda.com/childhood-celebrities-when-they-were-young-kids/?utm_source=google\u0026utm_medium=organic\u0026utm_campaign=organic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvqDOPXnLmkl"},"outputs":[],"source":["filenames_upload = os.listdir(os.path.join(upload_path))\n","filenames_upload.sort()\n","\n","filenames_upload_output = os.listdir(os.path.join(upload_output_path, \"final_output\"))\n","filenames_upload_output.sort()\n","\n","for filename, filename_output in zip(filenames_upload, filenames_upload_output):\n","    image_original = PIL.Image.open(os.path.join(upload_path, filename))\n","    image_restore = PIL.Image.open(os.path.join(upload_output_path, \"final_output\", filename_output))\n","\n","    display(make_grid(image_original, image_restore))\n","    print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"T2B75ztFYnnK"},"source":["## Download your results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pJxB6p1R1jE"},"outputs":[],"source":["output_folder = os.path.join(upload_output_path, \"final_output\")\n","print(output_folder)\n","os.system(f\"zip -r -j download.zip {output_folder}/*\")\n","files.download(\"download.zip\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdFXuH9qd5u9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA","timestamp":1735624687640}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}